{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e89e9f1",
   "metadata": {},
   "source": [
    "# HARP Register Deserialization Notebook\n",
    "This notebook reads `.bin` register data from the [Harp behavior device](https://github.com/harp-tech/device.behavior), parses it using  the [`harp-python`](https://github.com/harp-tech/harp-python) library, and creates timestamp-aligned dataframes. Individual and merged data are saved to disk.\n",
    "\n",
    "### 1. Import libraries, set data path and load data into a harp reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9175074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded device from: /Volumes/mrsic_flogel/public/projects/JeKr_130524_Joystick/rotary-encoder/bonsai_joystick_data/ML_166/allsessions/ses-001_date-20250918T161338/Behavior\n",
      "Found 5 registers:\n",
      "  ‚Ä¢ Behavior_842025-09-18T17_13_41.bin\n",
      "  ‚Ä¢ Behavior_82025-09-18T17_13_42.bin\n",
      "  ‚Ä¢ Behavior_442025-09-18T17_13_41.bin\n",
      "  ‚Ä¢ Behavior_922025-09-18T17_13_41.bin\n",
      "  ‚Ä¢ Behavior_342025-09-18T17_13_48.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import harp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_folder = \"/Volumes/mrsic_flogel/public/projects/JeKr_130524_Joystick/rotary-encoder/bonsai_joystick_data/ML_166/allsessions/ses-001_date-20250918T161338/Behavior\"\n",
    "# \"Z:/public/projects/JeKr_130524_Joystick/rotary-encoder/bonsai_joystick_data/JK_003/ses-001_date-20250917T104435/Behavior\"\n",
    "\n",
    "data_path = os.path.abspath(data_folder)\n",
    "deserialised_folder = os.path.join(data_path, r\"../Deserialised\")\n",
    "os.makedirs(deserialised_folder, exist_ok=True)\n",
    "\n",
    "reader = harp.create_reader(data_path)\n",
    "print(\"Loaded device from:\", data_folder)\n",
    "\n",
    "bin_files = glob.glob(os.path.join(data_path, \"Behavior_*.bin\"))\n",
    "print(f\"Found {len(bin_files)} registers:\")\n",
    "for f in bin_files:\n",
    "    print(\"  ‚Ä¢\", os.path.basename(f))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a119c",
   "metadata": {},
   "source": [
    "### 2. Load each registry's binary file and parse to a dictionary of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2aa76",
   "metadata": {},
   "source": [
    "First let's build a lookup table to easily read the name of each numbered register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec551d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_to_name = {\n",
    "    reg.register.address: name\n",
    "    for name, reg in reader.registers.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b6a9d",
   "metadata": {},
   "source": [
    "Now we can read each binary into a dataframe, and give them their correct names.\n",
    "\n",
    "1. Loop through list of files\n",
    "2. Find the register name from the LUT above\n",
    "3. Read the register bits, and convert to a dataframe\n",
    "4. Save each dataframe to a `.csv` (option to save as .pkl file commented out)\n",
    "5. Add each dataframe as an entry to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8909b6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport pandas as pd\\nfrom datetime import datetime\\n\\nall_dataframes = {}\\n\\n# --- Sort bin files by the timestamp in their name ---\\ndef extract_timestamp(filepath):\\n    \"\"\"Extract datetime from filename like Behavior_922025-10-15T17_34_13\"\"\"\\n    try:\\n        filename = os.path.basename(filepath)\\n        timestamp_str = filename.split(\\'-\\')[-1].replace(\\'.bin\\', \\'\\')\\n        return datetime.strptime(timestamp_str, \"%Y-%m-%dT%H_%M_%S\")\\n    except Exception:\\n        return datetime.min  # fallback if format unexpected\\n\\nbin_files_sorted = sorted(bin_files, key=extract_timestamp)\\n\\n# --- Process each .bin file in order ---\\nfor filepath in bin_files_sorted:\\n    filename = os.path.basename(filepath)\\n    parts = filename.split(\\'-\\')\\n\\n    # Extract register address\\n    reg_addr = int(parts[0][9:-4])\\n    register = reader.registers.get(reg_addr)\\n    df = register.read(filepath)\\n\\n    if df.empty:\\n        continue\\n\\n    # Register name\\n    reg_name = addr_to_name.get(reg_addr, f\"Register_{reg_addr}\")\\n    df.columns = [f\"{reg_name}_{col}\" for col in df.columns]\\n\\n    # Store in memory\\n    all_dataframes.setdefault(reg_name, []).append(df)\\n\\n    print(f\"‚úÖ Loaded {reg_name} from {filename}\")\\n\\n# --- Save each register once, in order ---\\nfor reg_name, dfs in all_dataframes.items():\\n    combined_df = pd.concat(dfs, ignore_index=True)\\n    out_csv_path = os.path.join(deserialised_folder, f\"{reg_name}.csv\")\\n    combined_df.to_csv(out_csv_path, index=False)\\n    print(f\"üíæ Saved combined {reg_name} ({len(combined_df)} rows) to {out_csv_path}\")\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "all_dataframes = {}\n",
    "\n",
    "# --- Sort bin files by the timestamp in their name ---\n",
    "def extract_timestamp(filepath):\n",
    "    \"\"\"Extract datetime from filename like Behavior_922025-10-15T17_34_13\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(filepath)\n",
    "        timestamp_str = filename.split('-')[-1].replace('.bin', '')\n",
    "        return datetime.strptime(timestamp_str, \"%Y-%m-%dT%H_%M_%S\")\n",
    "    except Exception:\n",
    "        return datetime.min  # fallback if format unexpected\n",
    "\n",
    "bin_files_sorted = sorted(bin_files, key=extract_timestamp)\n",
    "\n",
    "# --- Process each .bin file in order ---\n",
    "for filepath in bin_files_sorted:\n",
    "    filename = os.path.basename(filepath)\n",
    "    parts = filename.split('-')\n",
    "\n",
    "    # Extract register address\n",
    "    reg_addr = int(parts[0][9:-4])\n",
    "    register = reader.registers.get(reg_addr)\n",
    "    df = register.read(filepath)\n",
    "\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # Register name\n",
    "    reg_name = addr_to_name.get(reg_addr, f\"Register_{reg_addr}\")\n",
    "    df.columns = [f\"{reg_name}_{col}\" for col in df.columns]\n",
    "\n",
    "    # Store in memory\n",
    "    all_dataframes.setdefault(reg_name, []).append(df)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {reg_name} from {filename}\")\n",
    "\n",
    "# --- Save each register once, in order ---\n",
    "for reg_name, dfs in all_dataframes.items():\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    out_csv_path = os.path.join(deserialised_folder, f\"{reg_name}.csv\")\n",
    "    combined_df.to_csv(out_csv_path, index=False)\n",
    "    print(f\"üíæ Saved combined {reg_name} ({len(combined_df)} rows) to {out_csv_path}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051156a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m dfs = []\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     df = \u001b[43mregister\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m(f)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Group bin files by register address ---\n",
    "bins_by_register = defaultdict(list)\n",
    "for filepath in bin_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    parts = filename.split('-')\n",
    "    reg_addr = int(parts[0][9:-4])\n",
    "    bins_by_register[reg_addr].append(filepath)\n",
    "\n",
    "all_dataframes = {}\n",
    "\n",
    "# --- Loop through each register's group ---\n",
    "for reg_addr, files in bins_by_register.items():\n",
    "    # Sort by datetime extracted from filename (assuming second part is timestamp)\n",
    "    files.sort(key=lambda f: f.split('-')[1])\n",
    "\n",
    "    register = reader.registers.get(reg_addr)\n",
    "    reg_name = addr_to_name.get(reg_addr, f\"Register_{reg_addr}\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = register.read(f)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # ‚úÖ Only reset index for the OutputSet register\n",
    "        if \"OutputSet\" in reg_name and df.index.name is not None:\n",
    "            df = df.reset_index()\n",
    "\n",
    "        # ‚úÖ Always append dataframe (for all registers)\n",
    "        dfs.append(df)\n",
    "        print(f\"‚úÖ Read {reg_name} from {os.path.basename(f)} ({len(df)} rows)\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(f\"‚ö†Ô∏è No valid data found for {reg_name}\")\n",
    "        continue\n",
    "\n",
    "    # ‚úÖ Concatenate all bins for this register\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_df.columns = [f\"{reg_name}_{col}\" for col in combined_df.columns]\n",
    "\n",
    "    out_csv_path = os.path.join(deserialised_folder, f\"{reg_name}.csv\")\n",
    "    combined_df.to_csv(out_csv_path, index=False)\n",
    "\n",
    "    all_dataframes[reg_name] = combined_df\n",
    "    print(f\"üíæ Saved combined {reg_name} with {len(combined_df)} total rows\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa460a",
   "metadata": {},
   "source": [
    "## Troubleshooting - Checking the solenoid events are Timestamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "001b0864",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected address 34 but got 84",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Inspect the OutputSet dataframe structure\u001b[39;00m\n\u001b[32m      2\u001b[39m outputset_addr = [addr \u001b[38;5;28;01mfor\u001b[39;00m addr, name \u001b[38;5;129;01min\u001b[39;00m addr_to_name.items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mOutputSet\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name][\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregisters\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutputset_addr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbin_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head(\u001b[32m30\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bonsai_abcd/lib/python3.13/site-packages/harp/reader.py:90\u001b[39m, in \u001b[36m_compose_parser.<locals>.parser\u001b[39m\u001b[34m(data, columns, epoch, keep_type)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(\n\u001b[32m     85\u001b[39m     data: Optional[Union[_FileLike, _BufferLike]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     86\u001b[39m     columns: Optional[Axes] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     87\u001b[39m     epoch: Optional[datetime] = params.epoch,\n\u001b[32m     88\u001b[39m     keep_type: \u001b[38;5;28mbool\u001b[39m = params.keep_type,\n\u001b[32m     89\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     df = \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     result = f(df)\n\u001b[32m     92\u001b[39m     type_col = df.get(MessageType.\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bonsai_abcd/lib/python3.13/site-packages/harp/reader.py:179\u001b[39m, in \u001b[36m_create_register_reader.<locals>.reader\u001b[39m\u001b[34m(file_or_buf, columns, epoch, keep_type)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_or_buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m     file_or_buf = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams.base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregister.address\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.bin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m data = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bonsai_abcd/lib/python3.13/site-packages/harp/io.py:96\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(file_or_buf, address, dtype, length, columns, epoch, keep_type)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(\n\u001b[32m     89\u001b[39m         columns=columns,\n\u001b[32m     90\u001b[39m         index=pd.DatetimeIndex([], name=\u001b[33m\"\u001b[39m\u001b[33mTime\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m epoch\n\u001b[32m     92\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m pd.Index([], dtype=np.float64, name=\u001b[33m\"\u001b[39m\u001b[33mTime\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     93\u001b[39m     )\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m address \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m address != data[\u001b[32m2\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected address \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddress\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     98\u001b[39m index = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     99\u001b[39m stride = \u001b[38;5;28mint\u001b[39m(data[\u001b[32m1\u001b[39m] + \u001b[32m2\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: expected address 34 but got 84"
     ]
    }
   ],
   "source": [
    "# Inspect the OutputSet dataframe structure\n",
    "outputset_addr = [addr for addr, name in addr_to_name.items() if \"OutputSet\" in name][0]\n",
    "df = reader.registers[outputset_addr].read(bin_files[0])\n",
    "print(df.head(30))\n",
    "print(df.columns)\n",
    "\n",
    "print(f\"Saving {reg_name}: columns = {df.columns.tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfad597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = df.reset_index()  # moves 'Time' index into a column\\nprint(df.columns)\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "df = df.reset_index()  # moves 'Time' index into a column\n",
    "print(df.columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de80a5",
   "metadata": {},
   "source": [
    "### Loading Harp dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6d651",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f2350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_dataframes = {}\\n\\n# Load each register\\'s binary file in a loop\\nfor filepath in bin_files:\\n    # Grab registry address\\n    filename = os.path.basename(filepath)\\n    parts = filename.split(\\'-\\')\\n\\n    reg_addr = int(parts[0][9:-4]) # after behavior and before YYYY\\n    register = reader.registers.get(reg_addr)\\n\\n    # Read this register    \\n    df = register.read(filepath)\\n    if df.empty:\\n        continue\\n\\n    # Grab this register\\'s name\\n    reg_name = addr_to_name.get(reg_addr, f\"Register_{reg_addr}\")\\n    df.columns = [f\"{reg_name}_{col}\" for col in df.columns]\\n\\n    # Define save path\\n    out_csv_path = os.path.join(deserialised_folder, f\"{reg_name}.csv\")\\n    df.to_csv(out_csv_path, index=False)\\n\\n    # OR Save as .pkl\\n    # pkl_path = os.path.join(deserialised_folder, f\"{reg_name}.pkl\")\\n    # df.to_pickle(pkl_path)\\n\\n    all_dataframes[reg_name] = df\\n    print(f\"Saved {reg_name} to {out_csv_path}\")\\n    # print(f\"Saved {reg_name} to {pkl_path}\")\\n    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "all_dataframes = {}\n",
    "\n",
    "# Load each register's binary file in a loop\n",
    "for filepath in bin_files:\n",
    "    # Grab registry address\n",
    "    filename = os.path.basename(filepath)\n",
    "    parts = filename.split('-')\n",
    "    \n",
    "    reg_addr = int(parts[0][9:-4]) # after behavior and before YYYY\n",
    "    register = reader.registers.get(reg_addr)\n",
    "        \n",
    "    # Read this register    \n",
    "    df = register.read(filepath)\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # Grab this register's name\n",
    "    reg_name = addr_to_name.get(reg_addr, f\"Register_{reg_addr}\")\n",
    "    df.columns = [f\"{reg_name}_{col}\" for col in df.columns]\n",
    "\n",
    "    # Define save path\n",
    "    out_csv_path = os.path.join(deserialised_folder, f\"{reg_name}.csv\")\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "\n",
    "    # OR Save as .pkl\n",
    "    # pkl_path = os.path.join(deserialised_folder, f\"{reg_name}.pkl\")\n",
    "    # df.to_pickle(pkl_path)\n",
    "\n",
    "    all_dataframes[reg_name] = df\n",
    "    print(f\"Saved {reg_name} to {out_csv_path}\")\n",
    "    # print(f\"Saved {reg_name} to {pkl_path}\")\n",
    "    '''\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8221dbb",
   "metadata": {},
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06683ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe saved to Z:\\public\\projects\\JeKr_130524_Joystick\\rotary-encoder\\bonsai_joystick_data\\ML_165\\bestsessions\\ses-001_date-20251112T160916\\Behavior\\../Deserialised\\AllHarpEvents.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df = None\n",
    "for df in all_dataframes.values():\n",
    "    if merged_df is None:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, df, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "if merged_df is not None:\n",
    "    merged_df.sort_index(inplace=True)  # sort by datetime index\n",
    "    merged_csv_path = os.path.join(deserialised_folder, \"AllHarpEvents.csv\")\n",
    "    merged_df.to_csv(merged_csv_path)\n",
    "\n",
    "    # OR Save as .pkl\n",
    "    # merged_pkl_path = os.path.join(deserialised_folder, \"AllHarpEvents.pkl\")\n",
    "    # merged_df.to_pickle(merged_pkl_path)\n",
    "\n",
    "    print(f\"Merged dataframe saved to {merged_csv_path}\")\n",
    "    #print(f\"Merged dataframe saved to {merged_pkl_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353173f",
   "metadata": {},
   "source": [
    "### Merge dataframes more efficiently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4beb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport pandas as pd \\n\\n# Path to your deserialised folder\\ndeserialised_folder = r\"Z:\\\\public\\\\projects\\\\JeKr_130524_Joystick\\rotary-encoder\\x08onsai_joystick_data\\\\JK_008\\x08estsessions\\\\ses-001_date-20251015T163410\\\\Deserialised\"\\n\\n# Define expected files\\nfiles = {\\n    \"OutputSet\": os.path.join(deserialised_folder, \"OutputSet.csv\"),\\n    \"TimestampSeconds\": os.path.join(deserialised_folder, \"TimestampSeconds.csv\"),\\n    \"EncoderData\": os.path.join(deserialised_folder, \"EncoderData.csv\"),\\n}\\n\\ndfs = {}\\n\\n# --- 1Ô∏è‚É£ Load OutputSet (keep OutputSet_DO2 only) ---\\nif os.path.exists(files[\"OutputSet\"]):\\n    df_out = pd.read_csv(files[\"OutputSet\"], usecols=lambda c: \"OutputSet_DO2\" in c)\\n    dfs[\"OutputSet\"] = df_out\\n    print(f\"‚úÖ Loaded OutputSet ({df_out.shape[0]} rows)\")\\nelse:\\n    print(\"‚ö†Ô∏è OutputSet.csv not found\")\\n\\n# --- 2Ô∏è‚É£ Load TimestampSeconds ---\\nif os.path.exists(files[\"TimestampSeconds\"]):\\n    df_ts = pd.read_csv(files[\"TimestampSeconds\"])\\n    dfs[\"TimestampSeconds\"] = df_ts\\n    print(f\"‚úÖ Loaded TimestampSeconds ({df_ts.shape[0]} rows)\")\\nelse:\\n    print(\"‚ö†Ô∏è TimestampSeconds.csv not found\")\\n\\n# --- 3Ô∏è‚É£ Load EncoderData (only first two columns) ---\\nif os.path.exists(files[\"EncoderData\"]):\\n    df_enc = pd.read_csv(files[\"EncoderData\"], usecols=[0, 1])\\n    dfs[\"EncoderData\"] = df_enc\\n    print(f\"‚úÖ Loaded EncoderData ({df_enc.shape[0]} rows, first two columns only)\")\\nelse:\\n    print(\"‚ö†Ô∏è EncoderData.csv not found\")\\n\\n# --- 4Ô∏è‚É£ Merge all loaded DataFrames on their index (row-wise alignment) ---\\nif dfs:\\n    merged_df = pd.concat(dfs.values(), axis=1)\\n    merged_df.reset_index(drop=True, inplace=True)\\n\\n    merged_csv_path = os.path.join(deserialised_folder, \"AllHarpEvents.csv\")\\n    merged_df.to_csv(merged_csv_path, index=False)\\n    print(f\"üíæ Saved AllHarpEvents.csv with {merged_df.shape[0]} rows and {merged_df.shape[1]} columns\")\\nelse:\\n    print(\"‚ùå No dataframes loaded, check file paths or column names\")\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "# Path to your deserialised folder\n",
    "deserialised_folder = r\"Z:\\public\\projects\\JeKr_130524_Joystick\\rotary-encoder\\bonsai_joystick_data\\JK_008\\bestsessions\\ses-001_date-20251015T163410\\Deserialised\"\n",
    "\n",
    "# Define expected files\n",
    "files = {\n",
    "    \"OutputSet\": os.path.join(deserialised_folder, \"OutputSet.csv\"),\n",
    "    \"TimestampSeconds\": os.path.join(deserialised_folder, \"TimestampSeconds.csv\"),\n",
    "    \"EncoderData\": os.path.join(deserialised_folder, \"EncoderData.csv\"),\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# --- 1Ô∏è‚É£ Load OutputSet (keep OutputSet_DO2 only) ---\n",
    "if os.path.exists(files[\"OutputSet\"]):\n",
    "    df_out = pd.read_csv(files[\"OutputSet\"], usecols=lambda c: \"OutputSet_DO2\" in c)\n",
    "    dfs[\"OutputSet\"] = df_out\n",
    "    print(f\"‚úÖ Loaded OutputSet ({df_out.shape[0]} rows)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OutputSet.csv not found\")\n",
    "\n",
    "# --- 2Ô∏è‚É£ Load TimestampSeconds ---\n",
    "if os.path.exists(files[\"TimestampSeconds\"]):\n",
    "    df_ts = pd.read_csv(files[\"TimestampSeconds\"])\n",
    "    dfs[\"TimestampSeconds\"] = df_ts\n",
    "    print(f\"‚úÖ Loaded TimestampSeconds ({df_ts.shape[0]} rows)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TimestampSeconds.csv not found\")\n",
    "\n",
    "# --- 3Ô∏è‚É£ Load EncoderData (only first two columns) ---\n",
    "if os.path.exists(files[\"EncoderData\"]):\n",
    "    df_enc = pd.read_csv(files[\"EncoderData\"], usecols=[0, 1])\n",
    "    dfs[\"EncoderData\"] = df_enc\n",
    "    print(f\"‚úÖ Loaded EncoderData ({df_enc.shape[0]} rows, first two columns only)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è EncoderData.csv not found\")\n",
    "\n",
    "# --- 4Ô∏è‚É£ Merge all loaded DataFrames on their index (row-wise alignment) ---\n",
    "if dfs:\n",
    "    merged_df = pd.concat(dfs.values(), axis=1)\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    merged_csv_path = os.path.join(deserialised_folder, \"AllHarpEvents.csv\")\n",
    "    merged_df.to_csv(merged_csv_path, index=False)\n",
    "    print(f\"üíæ Saved AllHarpEvents.csv with {merged_df.shape[0]} rows and {merged_df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"‚ùå No dataframes loaded, check file paths or column names\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa826617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81752a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonsai_abcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
